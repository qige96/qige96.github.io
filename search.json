[{"title":"ML1","url":"/2020/03/12/ML1/","content":"\n**Q：机器学习中最简单的学习算法是什么？**\n\n最简单的机器学习算法莫过于**线性回归**算法了。线性回归算法的基本形式如下：\n$$\nf(x)=w_1 x_1+w_2 x_2+···+w_n x_n + b\n$$\n\n如果说把 $b$ 换成 $w_0$ ，我们能得到一个更简洁的公式\n\n$$\nf(x)=w_0 +w_2 x_2+···+w_n x_n\n$$\n\n其中x是自变量，w是参数。我们训练的目的就是调整w，使得我们输入一系列x值后得到的f(x)的（近似）真实值。一个常见的例子是房价预测例子。通过住房面积、房间个数两个特征来预测房价，则f(x)就是代表房价，x0=1,x1=住房面积，x2=房间个数。\n\n\n\n**Q：线性回归模型的参数w和b怎样确定？**\n\n一个预测模型的效果越好，意味着其预测误差越小。那我们的任务就是找出使得预测值f(x)与真实值y的差最小的参数w。那么我们可以建立一个函数：\n$$\nJ(w) = \\sum_i [f(x_i|w,b)-y_i ]^2\n$$\n\n这个其实是在取定参数w和b的情况下，把每一次的预测值与真实值的差的平方加起来，得到了总的均方误差。令这个函数取最小值，那么就得到了能让f(x)与y的差最小，亦即我们的预测值与真实值的总误差最小的参数w和b，我们的回归模型效果也就最好了。\n\n现在问题转化为最小化上面的损失函数 $J(w, b)$。那么这个函数的最小化又要怎么做呢？\n\n高中的数学告诉我们，求一个函数的最小值，可以先求导，令导数为0，就找到了一堆极值点。然后把极值点代入原函数，算出函数值最小的点就是对应的最小值点了。\n\n![求导找最值](https://upload-images.jianshu.io/upload_images/3139341-a6924cd4f3432c56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![w的最优值](https://upload-images.jianshu.io/upload_images/3139341-d6ab903a178ecac2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n![b的最值](https://upload-images.jianshu.io/upload_images/3139341-c518ffa3921d6624.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n因为损失函数 $J(w, b)$是一个凸函数，所以求出来的解直接就是最优解了。如此我们就获得了一个可以使用的线性回归模型。\n\n\n\n**Q：线性回归模型是用来做预测的，但做分类任务时应该用那一个模型？**\n\n上面提到的线性模型是最基本的线性模型，把这个基本线性模型加以改造，就可以得到用于分类任务的回归模型。\n\n假设我们输入一组住房面积和房间个数的数据，预测到了一组住房的房价，比如A房80万，B房135万，C房150万等。如果我们认为画一条线，认为140万以下为低档房，140万以上为高档房，那我们就可以根据住房面积和房间个数来给住房分类了。也就是说在预测的基础上加上一个判断条件，就构成了分类算法。\n\n对数几率回归模型，或者称逻辑回归（Logit Regression）就是这样一种基于回归的分类算法。\n$$\nh_w(x) = g(w^Tx) = \\frac{1}{1+e^{-(w_0+w_1x_1...+w_nx_n)}}\n$$\n\n> 一般来说中文对Logistic Regression的翻译多是“逻辑斯蒂回归”，也就是音译，不过周志华老师这里的“对数几率回归”则是意译（又有对数，又有概率解释，还是回归方法）。然而我自己也没找到Logistic Regression这个词的内涵，维基百科也是：Verhulst did not explain the choice of the term \"logistic\", but it is presumably in contrast to the *logarithmic* curve, and by analogy with arithmetic and geometric.\n\n对数几率回归函数输出的的值是0到1范围的数。对于一个二分类任务来说，假如只分正例和反例（好瓜和坏瓜），那么我们可以认为函数输出的值越大，则这个样本是正例的几率越大，值越小，样本是反例的可能越大。这就是“对数几率回归”名字的又来。我们可以定一个临界值，一般是0.5，当输出值大于0.5时认为该样本是正例，否则为反例。\n\n\n\n**Q：对数几率回归的参数w和b又是怎样确定的？**\n\n我们知道，对数几率回归函数的输出值是一个概率。那么一个对率回归模型越好，意味着当输入模型的样本为正例时，模型的输出值越大（越接近1），当输入模型的样本为反例时，模型的输出值越小（越接近0）。\n\n因此我们可以构建这样一个函数，具体推到参考[这篇文章](https://zhuanlan.zhihu.com/p/33543849)\n$$\nl(w)=\\sum_{i=1}^{m}[y_iln(h_w(x))+(1-y_i)ln(1-h_w(x))]\n$$\n上式中$h_w(x)$是对率模型的输出值，亦即x为正例的概率值，yi是样本的真实标记，当样本为正例时y取1，为反例时y取0。这整一个函数的含义就是把每个样本属于其真实标记的概率加起来。若我们使 l(w,b) 最大，那就可以找到一组θ使得每个样本属于其真实标记的概率最大，亦即使得正例的函数值尽可能接近1，反例的函数值尽可能接近0。现在问题又转化成了最优化问题，只是这一次得函数没有上面线性回归函数那么好，很难用求导的方式确定最优解。\n\n其实最优化问题是一类经典的工程数学问题，现在已经有了许多种成熟的更泛用解决方案，比如随机游走，比如遗传算法，但在机器学习领域最常用的还是**梯度下降**方法。可能是因为梯度下降比较容易理解，效果也比较确定吧？\n\n“梯度下降”中的梯度又是一个什么东西？梯度是一个向量，其方向表示函数变化最快的方向，其大小（模）表示函数在此方向上的变化率。计算函数在某一点上的梯度，就是要算出函数在该点上的所有维度上的偏导数。简单理解就是，梯度是一元函数下的导数在多元函数下的推广（虽然并不准确）。\n\n![weikipedia image](https://upload-images.jianshu.io/upload_images/3139341-790e9c00a39b1fc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n梯度下降法是一个迭代进行的方法，每一轮都朝着梯度方向走动一小步，逐渐逼近最优值。\n\n```\n// psudocode for gradient descent\nprocedure grad_desc(X, y, w, learning_rate, precision)\n    prev_w = w\n    do\n    \tw = w + learning_rate * gradient(w, X, y)\n    while f(prev_w) - f(w) < precision\n    return w\nend\n```\n\n如此我们就得到了一个优秀的对数几率回归模型。\n\n\n\n**Q：对数几率回归模型是处理二分类任务的，那处理多分类任务时怎么办？**\n\n多分类任务可以凭借二分类任务为基础得到解决。解决思路一般有一对一，一对多，和多对多三种。\n\n**一对一策略**：举个例子，现在有A、B、C、D四个种类，要确定某个样本属于四类中的哪一类。那么我们可以事先训练好六个二分类的分类器——A/B、A/C、A/D、B/C、B/D、C/D。然后把要确定类别的样本分别放入这六个分类器。假设分类结果分别是A、A、A、B、D、C。可以知道六个分类器中有三个认为这个样本术语A，认为是B、C、D的各有一个。所以我们可以认为这个样本就是术语A类的。\n\n**一对多策略**：举个例子，现在有A、B、C、D四个种类，要确定某个样本属于四类中的哪一类。那么我们可以事先训练好四个二分类的分类器——A/BCD、B/ACD、C/ABD、D/ABC,分类器输出的是一个函数值。然后把要确定类别的样本分别放入这四个分类器。假设四个分类器的结果分别是“属于A的概率是0.9”，“属于B的概率是0.8”、“属于C的概率是0.7”、“属于B的概率是0.6”。那我们可以认为这个样本是属于A。\n\n**多对多策略**：每次将若干个类作为正类，若干个其他类作为反类。其中需要这种策略需要特殊的设计，不能随意取。常用的技术：纠错输出码。工作步骤分为：\n\n- 编码：对N个类别做M次划分，每次划分将一部分类别作为正类，一部分划分为反类，从而形成一个二分类训练集；这样一共产生M个训练集，可以训练出M个分类器。\n- 解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别最为最终预测结果。\n\n![Paste_Image.png](http://upload-images.jianshu.io/upload_images/3139341-caf3df29235c1892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n以原书的例子做一个详细的演示：\n假如现在有一个训练数据集，可以分四个类——C1, C2, C3, C4\n再具体一点可以想像为——西瓜可以分为一等瓜、二等瓜、三等瓜、四等瓜，要训练一个分类系统，使之能判断一个西瓜的等级。\n\n我们对训练数据集做五次划分\n\n- 第一次，标记C2为正例，其他为反例，训练出一个二分类的分类器f1\n- 第二次，标记C1、C3为正例，其他为反例，训练出一个二分类的分类器f2\n- 第三次，标记C3、C4为正例，其他为反例，训练出一个二分类的分类器f3\n- 第四次，标记C1、C2、C4为正例，其他为反例，训练出一个二分类的分类器f4\n- 第五次，标记C1、C4为正例，其他为反例，训练出一个二分类的分类器f5\n\n根据这五次划分的过程，每一个类都获得了一个编码（向量）：\n\n- C1：（-1，1，-1，1，1）\n- C2：（1，-1，-1，1，-1）\n- C3：（-1，1，1，-1，1）\n- C4：（-1，-1，1，1，-1）\n\n若现在有一个测试样本，五个分类器对应的累计结果为\nf1：反，  f2：反，  f3：正， f4：反， f5：正\n即该测试样本对应的编码/向量为（-1，-1，1，-1，1）\n那么分别计算这个测试样本的编码与四个类别的编码的向量距离，可以使用欧氏距离，算的与C3类的距离是最小的。因此判定该测试样本属于C3类。\n\n---\n\n>  Talk is cheap, show me the code!\n\n先来一段Linear Regression的代码\n\n```python\n\"\"\"\nSimple linear regression\nAuthor: Richy Zhu\nEmail: rickyzhu@foxmail.com\n\"\"\"\nimport numpy as np\n# 不用numpy也行，只是会很麻烦，而且效率也会大大降低\n\ndef _loss_function(X, y, w):\n    y_hat = X.dot(w)\n    return np.sum(y - y_hat)**2\n\ndef _gradient_descent(X, y, w, eta):\n    y_hat = X.dot(w)\n    diff = y - y_hat\n    grad = eta * diff.reshape([-1,1]) * X * 1.0/len(X)\n    w += grad.sum(axis=0)\n    return w\n\ndef linear_regression(X, y, eta=0.00001, epslon=0.001, max_iter=100000):\n    \"\"\"\n    X        - (m, n) matrix, m for number of instances, and n for number of features\n    y        - m vector\n    eta      - float number, learning rate\n    epslon   - float number, stopping criteria\n    max_iter - max iteration\n    \"\"\"\n    X = np.hstack([X, np.ones([len(X),1])]) # add bias\n    # w = np.random.random(X.shape[1]) \n    w = np.ones(X.shape[1])  # standard practice is to randomly initialize\n    prev_loss =  _loss_function(X, y, w)\n    for i in range(max_iter):\n        w = _gradient_descent(X, y, w, eta)\n        this_loss =  _loss_function(X, y, w)\n        if prev_loss - this_loss < epslon:\n            break\n        prev_loss =  this_loss\n    return w\n\n```\n\n测试一下\n\n```python\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\nw = linear_regression(X_train, y_train, eta=0.000001, epslon=0.0001)\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nprint(mean_squared_error(y_test,  np.hstack([X_test, np.ones([len(X_test),1])]).dot(w)))\nprint(mean_squared_error(y_test, lr.predict(X_test)))\n\n```\n\n下面是Logistic Regression的代码\n\n```python\n\"\"\"\nSimple Logistic Regression\nAuthor: Richy Zhu\nEmail: rickyzhu@foxmail.com\n\"\"\"\nimport numpy as np\n\ndef binarize(y):\n    return np.where(y<0.5, 0, 1)\n\ndef sigmoid(X): \n   return 1 / (1 + np.exp(-X))\n\ndef _loss_function(X, y, w): \n   y_hat = sigmoid(X.dot(w))\n   ll = np.sum(y * np.log(y_hat) - (1-y) * np.log(1-y_hat))\n   return ll\n\ndef _gradient_descent(X, y, w, eta):\n    y_hat = sigmoid(X.dot(w))\n    diff = y - y_hat\n    grad = eta * diff.reshape([-1,1]) * X * 1.0/len(X)\n    w += grad.sum(axis=0)\n    return w\n\ndef logistic_regression(X, y, eta=0.00001, epslon=0.001, max_iter=100000):\n    \"\"\"\n    X        - (m, n) matrix, m for number of instances, and n for number of features\n    y        - m vector\n    eta      - float number, learning rate\n    epslon   - float number, stopping criteria\n    max_iter - max iteration\n    \"\"\"\n    X = np.hstack([X, np.ones([len(X),1])]) # add bias\n#     w = np.random.random(X.shape[1]) \n    w = np.ones(X.shape[1])  # standard practice is to randomly initialize\n    prev_loss =  _loss_function(X, y, w)\n    for i in range(max_iter):\n        w = _gradient_descent(X, y, w, eta)\n        this_loss = _loss_function(X, y, w)\n        if prev_loss - this_loss < epslon:\n            break\n        prev_loss = this_loss\n    return w\n\n```\n\n测试一下\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nX = iris.data[iris.target!=2]   # only use samples with class 0 or 1\ny = iris.target[iris.target!=2] # only use samples with class 0 or 1\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nw = logistic_regression(X_train, y_train, eta=0.0001, epslon=0.0001)\n\nfrom sklearn.linear_model import LogisticRegression\nlgr = LogisticRegression()\nlgr.fit(X_train, y_train)\n\nprint(accuracy_score(y_test, binarize(sigmoid(np.hstack([X_test, np.ones([len(X_test),1])]).dot(w)))))\nprint(accuracy_score(y_test, lgr.predict(X_test)))\n\nprint(sigmoid(np.hstack([X_test, np.ones([len(X_test),1])]).dot(w)))\nprint(y_test)\n\n```\n\n\n\n以scikit-learn的结果为准/(ㄒoㄒ)/~~\n\n---\n\n本作品首发于[简书](https://www.jianshu.com/u/df4c5558ec8a) 和 [博客园](https://www.cnblogs.com/zrq96/)平台，采用[知识共享署名 4.0 国际许可协议](https://creativecommons.org/licenses/by/4.0/)进行许可。\n"},{"title":"hello","url":"/2020/03/01/hello/","content":"\nhello\n"}]